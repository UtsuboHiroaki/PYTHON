# ウェブからのデータ取得

## 全体の流れ:

1. ウェブサーバに対して「リクエスト」を発行する。
2. ウェブサーバから「レスポンス」を受け取る。
3. レスポンスを解析する。

レスポンスは文字列データとして得られる。
なので、前章で学んだ、CSVファイル、JSONファイルの解析手法がそのまま使える。

レスポンスがHTML形式(≒XML形式)の場合は、その中身をゴリゴリ解析する。  
「スクレイピング」とは、この「ゴリゴリ解析」のこと。

スクレイプ: 「表面を削り取る」的な意味。アイスクリームを削り取る器具などは、「スクレイパー」ですね。

## ウェブからのデータ取得で使う、代表的なライブラリ

ウェブからデータを取得する方法として、代表的なのは以下の2つ

| 手法 | 特徴 |
| ---- | ---- |
| urllib.request | pip 不要ですぐに使える |
| requests | pip install requests が必要だが、利便性は urllib.request より上。推奨 |

## ウェブからのデータ取得でよくあるデータ形式

ウェブから取ってくるデータの代表的な形式は、以下の3つ

| 形式   | 概要                        | メモ |
|------|---------------------------| ---- |
| CSV  | 表形式のものデータ。                | 表形式ならまずこれ |
| JSON | 外部のプログラムに読ませるために準備されたデータ。 | 構造を有するデータならまずこれ | 
| XML  | 外部のプログラムに読ませるために準備されたデータ。 | 構造を有するデータでは、かつては XML か主流だった | 
| HTML | ブラウザで表示させるために準備されたデータ。    | 人間に見せるようのページのデータ。いわゆる「スクレイピング」の対象 |

JSONとXMLでは、JSONのほうが今日では一般的。  
XMLは、HTMLをスクレイピングする要領で解析する。

## 取得したデータの解析

JSONの解析方法についてはすでに述べたとおり。  
XML, HTML を解析する方法として、代表的なのは以下の2つ

| 手法 | 特徴                                              |
| ---- |-------------------------------------------------|
| xml.etree.ElementTreeモジュール | pip 不要ですぐに使える                                   |
| BeautifulSoupモジュールを使う | pip install bs4 が必要だが、利便性は ElementTreeモジュール より上。推奨 |

### データ解析の基本文法

XML, HTML の「タグ」、「属性」を元にして絞りこんでいく。
CSS(*1) の queryselector という記法で絞りこむ。

(*1)
「Cascading Style Sheet」の略。  
「スタイルシート」、つまり、HTMLページの体裁を指定するファイル。

HTMLファイルの解析で、まず覚えたいのは、以下の3つ。

- タグ
- id
- class
- 属性

| 記法 | 概要 | 戻り値の型 |
| ---- | ---- |-------|
| .find() | タグ名で絞り込む | Tagオブジェクト |
| .find_all() | タグ名で絞り込む | Tagオブジェクトのリスト |
| .find_all(属性名="属性値") | 属性名と属性値で絞り込む | Tagオブジェクトのリスト |
| .select() | CSSのセレクタで絞り込む | Tagオブジェクトのリスト |
| .select_one() | CSSのセレクタで絞り込む | Tagオブジェクト |





